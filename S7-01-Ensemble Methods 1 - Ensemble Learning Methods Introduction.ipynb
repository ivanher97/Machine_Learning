{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b458308",
   "metadata": {},
   "source": [
    "# ü§ñ M√©todos de Aprendizaje por Ensamble ‚Äî Introducci√≥n\n",
    "\n",
    "## üß© ¬øQu√© es el Aprendizaje por Ensamble?\n",
    "\n",
    "El **aprendizaje por ensamble (ensemble learning)** es una t√©cnica de **aprendizaje supervisado** que combina varios modelos (tambi√©n llamados *predictores*, *modelos base* o *aprendices d√©biles*) para formar un **modelo final m√°s fuerte y robusto** (*strong learner*).\n",
    "\n",
    "üëâ La idea principal es:  \n",
    "> ‚ÄúVarios modelos d√©biles, combinados adecuadamente, pueden formar un modelo fuerte.‚Äù\n",
    "\n",
    "- Se aplica tanto a **clasificaci√≥n** como a **regresi√≥n**.  \n",
    "- Los modelos base pueden ser de cualquier tipo: **√°rboles de decisi√≥n, redes neuronales, SVM, etc.**\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Principales T√©cnicas de Ensamble\n",
    "\n",
    "### 1. **Bagging (Bootstrap Aggregating)**\n",
    "- **Objetivo:** Reducir la varianza y evitar el sobreajuste (*overfitting*).\n",
    "- **C√≥mo funciona:**\n",
    "  - Se crean **muestras aleatorias con reemplazo** del conjunto de datos (*bootstrap samples*).\n",
    "  - Se entrena un **modelo diferente** en cada muestra.\n",
    "  - Las predicciones se combinan:\n",
    "    - **Regresi√≥n:** Promedio de las salidas.\n",
    "    - **Clasificaci√≥n:** Votaci√≥n por mayor√≠a.\n",
    "\n",
    "- **Caracter√≠sticas:**\n",
    "  - Se puede aplicar a muchos modelos (por ejemplo, √°rboles de decisi√≥n o redes neuronales).\n",
    "  - Los modelos se entrenan **de forma independiente y en paralelo**.\n",
    "  - Reduce la **varianza**, pero no necesariamente el **sesgo**.\n",
    "\n",
    "- **Ejemplo en Scikit-Learn:**  \n",
    "  `RandomForestClassifier` o `BaggingClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Pasting**\n",
    "- Similar al *Bagging*, pero:\n",
    "  - Usa **muestreo sin reemplazo**.\n",
    "- Tiene un comportamiento estad√≠stico ligeramente distinto, pero busca el mismo objetivo: reducir la varianza.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Boosting**\n",
    "- **Objetivo:** Convertir varios modelos d√©biles en un modelo fuerte.\n",
    "- **C√≥mo funciona:**\n",
    "  1. Entrena un modelo base sobre los datos.\n",
    "  2. Eval√∫a los errores y **aumenta el peso** de los ejemplos mal clasificados.\n",
    "  3. Entrena un nuevo modelo que se enfoca en esos ejemplos dif√≠ciles.\n",
    "  4. Combina todos los modelos ponderando seg√∫n su **precisi√≥n**.\n",
    "\n",
    "- **Efecto:** Obliga al algoritmo a aprender m√°s de los errores.\n",
    "\n",
    "- **Excepciones:**  \n",
    "  En m√©todos como *Boost by Majority* o *BrownBoost*, los ejemplos mal clasificados repetidamente pueden **disminuir su peso**.\n",
    "\n",
    "- **Ejemplos populares:**\n",
    "  - **AdaBoost**\n",
    "  - **Gradient Boosting**\n",
    "  - **XGBoost**, **LightGBM**, **CatBoost**\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Stacking (Apilamiento o Generalizaci√≥n Apilada)**\n",
    "- **Objetivo:** Combinar m√∫ltiples modelos *diferentes* para obtener un modelo final m√°s estable y generalizable.\n",
    "- **Concepto:**\n",
    "  - Los **modelos de nivel 1 (base)** se entrenan primero.\n",
    "  - Un **modelo de nivel 2 (meta-modelo)** aprende a combinar sus predicciones.\n",
    "\n",
    "- **Pasos:**\n",
    "  1. Entrenar varios modelos base (por ejemplo, √°rbol de decisi√≥n, regresi√≥n log√≠stica, SVM).\n",
    "  2. Generar predicciones con esos modelos.\n",
    "  3. Entrenar un *meta-modelo* que use esas predicciones como variables de entrada.\n",
    "\n",
    "- **Ventajas:**\n",
    "  - Funciona mejor cuando los modelos base son **muy diferentes**.\n",
    "  - Reduce los puntos d√©biles de cada modelo individual.\n",
    "  - Es una t√©cnica **muy usada en competiciones de Kaggle**.\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Otros M√©todos de Ensamble\n",
    "\n",
    "### üîπ Clasificador Bayesiano √ìptimo\n",
    "- Es el ensamble **te√≥ricamente m√°s preciso** posible.\n",
    "- Considera **todas las hip√≥tesis** posibles, ponderadas por:\n",
    "  - La probabilidad de que los datos provengan de esa hip√≥tesis.\n",
    "  - Su probabilidad *a priori*.\n",
    "- No se usa en la pr√°ctica, porque requiere conocer todo el espacio de hip√≥tesis.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Promediado de Modelos Bayesianos (BMA)\n",
    "- Una aproximaci√≥n pr√°ctica al clasificador bayesiano √≥ptimo.\n",
    "- **Muestrea hip√≥tesis** del espacio de modelos y las combina usando la **ley de Bayes**.\n",
    "- Se implementa usando m√©todos Monte Carlo (por ejemplo, **MCMC**).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Combinaci√≥n de Modelos Bayesianos (BMC)\n",
    "- Mejora el BMA al **muestrear combinaciones de modelos** en lugar de modelos individuales.\n",
    "- Los pesos se extraen de una **distribuci√≥n de Dirichlet**.\n",
    "- Aunque es m√°s costoso computacionalmente, suele dar **mejores resultados** que BMA y Bagging.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Conjunto o ‚ÄúBucket‚Äù de Modelos\n",
    "- Usa un algoritmo de selecci√≥n de modelos para elegir **el mejor modelo** seg√∫n el problema.\n",
    "- Para un solo problema ‚Üí rinde igual que el mejor modelo.  \n",
    "- En muchos problemas ‚Üí mejora el rendimiento promedio general.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Paquetes de R para Ensambles Bayesianos\n",
    "- **BMS**: *Bayesian Model Selection*  \n",
    "- **BAS**: *Bayesian Adaptive Sampling*  \n",
    "- **BMA**: *Bayesian Model Averaging*\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Puntos Clave\n",
    "\n",
    "- Los m√©todos de ensamble funcionan mejor con **predictores independientes**.\n",
    "- Combinar **diferentes tipos de algoritmos** (por ejemplo, √°rbol + lineal + red neuronal) suele producir mejores resultados.\n",
    "- Principales ventajas:\n",
    "  - Aumentan la **precisi√≥n**.\n",
    "  - Reducen la **varianza**.\n",
    "  - Incrementan la **robustez** del modelo.\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
