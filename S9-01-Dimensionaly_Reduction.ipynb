{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0577bdae",
   "metadata": {},
   "source": [
    "# Reducción de Dimensionalidad\n",
    "\n",
    "## Análisis de Componentes Principales (PCA)\n",
    "* Reducción de dimensionalidad lineal utilizando la Descomposición en Valores Singulares de los datos para proyectarlos a un espacio de menor dimensión.\n",
    "* [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
    "* Procedimiento estadístico que utiliza la tecnología de [transformación ortogonal](https://en.wikipedia.org/wiki/Orthogonal_transformation)\n",
    "* Convierte características (predictores) posiblemente correlacionadas en características (predictores) **linealmente no correlacionadas** llamadas **componentes principales**.\n",
    "* Número de componentes principales <= número de características (predictores).\n",
    "* El primer componente principal explica la **mayor varianza posible**.\n",
    "* Cada componente subsiguiente tiene la mayor varianza sujeto a la restricción de que debe ser ortogonal a los componentes precedentes.\n",
    "* Una colección de los componentes se llama **vectores**.\n",
    "* **Sensible al escalado**.\n",
    "* [Sebastian Raschka](http://sebastianraschka.com/Articles/2014_python_lda.html): Ejes de componentes que maximizan la varianza.\n",
    "\n",
    "---\n",
    "\n",
    "## Análisis Discriminante Lineal (LDA)\n",
    "* [Wikipedia](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)\n",
    "* [Sebastian Raschka](http://sebastianraschka.com/Articles/2014_python_lda.html)\n",
    "* Se utiliza más comúnmente como técnica de reducción de dimensionalidad en el paso de preprocesamiento para aplicaciones de clasificación de patrones y aprendizaje automático.\n",
    "* El objetivo es proyectar un conjunto de datos en un espacio de menor dimensión con **buena separabilidad de clases** para evitar el sobreajuste (\"maldición de la dimensionalidad\") y también reducir los costes computacionales.\n",
    "* Localiza los \"**límites**\" alrededor de los clústeres de clases.\n",
    "* Proyecta los puntos de datos en una línea.\n",
    "* Se asignará un **centroide** a cada clúster o habrá un centroide cerca.\n",
    "* [Sebastian Raschka](http://sebastianraschka.com/Articles/2014_python_lda.html): Maximiza los ejes de componentes para la separación de clases.\n",
    "\n",
    "### Otras Técnicas de Reducción de Dimensionalidad\n",
    "\n",
    "* [Escalado Multidimensional (MDS)](http://scikit-learn.org/stable/modules/manifold.html#multi-dimensional-scaling-mds)\n",
    "    * Busca una representación de baja dimensión de los datos en la que las distancias respeten bien las distancias en el espacio original de alta dimensión.\n",
    "\n",
    "* [Isomap (Mapeo Isométrico)](http://scikit-learn.org/stable/modules/manifold.html#isomap)\n",
    "    * Busca una incrustación de menor dimensión que mantenga las distancias geodésicas entre todos los puntos.\n",
    "\n",
    "* [Incrustación Estocástica de Vecinos T-distribuidos (t-SNE)](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)\n",
    "    * Técnica de reducción de dimensionalidad no lineal que es particularmente adecuada para incrustar datos de alta dimensión en un espacio de dos o tres dimensiones, que luego se puede visualizar en un gráfico de dispersión.\n",
    "    * Modela cada objeto de alta dimensión mediante un punto bidimensional o tridimensional de tal manera que los objetos similares se modelan mediante puntos cercanos y los objetos **disímiles** se modelan mediante **puntos distantes** (por ejemplo, para visualizar las imágenes MNIST en 2D).\n",
    "\n",
    "***\n",
    "\n",
    "# Introducción Suave al Álgebra Lineal\n",
    "\n",
    "Revisión de Álgebra Lineal:\n",
    "\n",
    "$$A=\\begin{bmatrix} 1. & 2. \\\\ 10. & 20. \\end{bmatrix}$$\n",
    "\n",
    "$$B=\\begin{bmatrix} 1. & 2. \\\\ 100. & 200. \\end{bmatrix}$$\n",
    "\n",
    "\\begin{align}\n",
    "A \\times B & = \\begin{bmatrix} 1. & 2. \\\\ 10. & 20. \\end{bmatrix} \\times \\begin{bmatrix} 1. & 2. \\\\ 100. & 200. \\end{bmatrix} \\\\\n",
    "& = \\begin{bmatrix} 201. & 402. \\\\ 2010. & 4020. \\end{bmatrix} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Por partes:\n",
    "$$A \\times B = \\begin{bmatrix} 1. \\times 1. + 2.  \\times 100. &  1. \\times 2. + 2. \\times 200. \\\\ \n",
    "10. \\times 1. + 20. \\times 100. & 10. \\times 2. + 20. \\times 200. \\end{bmatrix}$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
